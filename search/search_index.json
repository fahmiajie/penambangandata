{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Tugas 3 Pengertian Seleksi Fitur Seleksi fitur adalah teknik untuk memilih fitur penting dan relevan terhadap data dan mengurangi fitur yang tidak relevan. Seleksi fitur bertujuan untuk memilih fitur terbaik dari suatu kumpulan data fitur. Seleksi fitur bertujuan untuk memilih fitur terbaik dari suatu kumpulan data fitur. Tujuan dari penelitian ini adalah menerapkan metode Information Gain dalam sistem seleksi fitur untuk Permasalahan cuaca . Metode Information Gain adalah metode yang menggunakan teknik scoring untuk pembobotan sebuah fitur dengan menggunakan maksimal entropy. Fitur yang dipilih adalah fitur dengan nilai Information Gain yang lebih besar atau sama dengan nilai threshold tertentu. contoh data permasalahan cuaca Kita dapat menghitung \"seberapa berharga\" fitur X dalam data melalui Feature Gain. Dengan demikian, fitur terlalu banyak bisa dikurangi. Mencari Entropy Target from pandas import * from IPython.display import HTML, display from tabulate import tabulate from math import log from sklearn.feature_selection import mutual_info_classif df = read_csv('feature selection.csv') df outlook temperature humidity windy play 0 sunny hot high False no 1 sunny hot high True no 2 overcast hot high False yes 3 rainy mild high False yes 4 rainy cool normal False yes 5 rainy cool normal True no 6 overcast cool normal True yes 7 sunny mild high False no 8 sunny cool normal False yes 9 rainy mild normal False yes 10 sunny mild normal True yes 11 overcast mild high True yes 12 overcast hot normal False yes 13 rainy mild high True no def findEntropy(column): rawGroups = df.groupby(column) targetGroups = [[key, len(data), len(data)/df[column].size] for key,data in rawGroups] targetGroups = DataFrame(targetGroups, columns=['value', 'count', 'probability']) return sum([-x*log(x,2) for x in targetGroups['probability']]), targetGroups, rawGroups entropyTarget, groupTargets, _ = findEntropy('play') table(groupTargets) print('entropy target =', entropyTarget) value count probability no 5 0.357143 yes 9 0.642857 entropy target = 0.9402859586706309 def findGain(column): entropyOutlook, groupOutlooks, rawOutlooks = findEntropy(column) table(groupOutlooks) gain = entropyTarget-sum(len(data)/len(df)*sum(-x/len(data)*log(x/len(data),2) for x in data.groupby('play').size()) for key,data in rawOutlooks) print(\"gain of\",column,\"is\",gain) return gain gains = [[x,findGain(x)] for x in ['outlook','temperature','humidity','windy']] value count probability overcast 4 0.285714 rainy 5 0.357143 sunny 5 0.357143 gain of outlook is 0.2467498197744391 value count probability cool 4 0.285714 hot 4 0.285714 mild 6 0.428571 gain of temperature is 0.029222565658954647 value count probability high 7 0.5 normal 7 0.5 gain of humidity is 0.15183550136234136 value count probability False 8 0.571429 True 6 0.428571 gain of windy is 0.04812703040826927 columns=[\"Feature\", \"Gain Score\"]).sort_values(\"Gain Score\")[::-1]) Feature Gain Score outlook 0.24675 humidity 0.151836 windy 0.048127 temperature 0.0292226","title":"Tugas 3"},{"location":"#tugas-3","text":"","title":"Tugas 3"},{"location":"#pengertian-seleksi-fitur","text":"Seleksi fitur adalah teknik untuk memilih fitur penting dan relevan terhadap data dan mengurangi fitur yang tidak relevan. Seleksi fitur bertujuan untuk memilih fitur terbaik dari suatu kumpulan data fitur. Seleksi fitur bertujuan untuk memilih fitur terbaik dari suatu kumpulan data fitur. Tujuan dari penelitian ini adalah menerapkan metode Information Gain dalam sistem seleksi fitur untuk Permasalahan cuaca . Metode Information Gain adalah metode yang menggunakan teknik scoring untuk pembobotan sebuah fitur dengan menggunakan maksimal entropy. Fitur yang dipilih adalah fitur dengan nilai Information Gain yang lebih besar atau sama dengan nilai threshold tertentu. contoh data permasalahan cuaca Kita dapat menghitung \"seberapa berharga\" fitur X dalam data melalui Feature Gain. Dengan demikian, fitur terlalu banyak bisa dikurangi.","title":"Pengertian Seleksi Fitur"},{"location":"#mencari-entropy-target","text":"from pandas import * from IPython.display import HTML, display from tabulate import tabulate from math import log from sklearn.feature_selection import mutual_info_classif df = read_csv('feature selection.csv') df outlook temperature humidity windy play 0 sunny hot high False no 1 sunny hot high True no 2 overcast hot high False yes 3 rainy mild high False yes 4 rainy cool normal False yes 5 rainy cool normal True no 6 overcast cool normal True yes 7 sunny mild high False no 8 sunny cool normal False yes 9 rainy mild normal False yes 10 sunny mild normal True yes 11 overcast mild high True yes 12 overcast hot normal False yes 13 rainy mild high True no def findEntropy(column): rawGroups = df.groupby(column) targetGroups = [[key, len(data), len(data)/df[column].size] for key,data in rawGroups] targetGroups = DataFrame(targetGroups, columns=['value', 'count', 'probability']) return sum([-x*log(x,2) for x in targetGroups['probability']]), targetGroups, rawGroups entropyTarget, groupTargets, _ = findEntropy('play') table(groupTargets) print('entropy target =', entropyTarget) value count probability no 5 0.357143 yes 9 0.642857 entropy target = 0.9402859586706309 def findGain(column): entropyOutlook, groupOutlooks, rawOutlooks = findEntropy(column) table(groupOutlooks) gain = entropyTarget-sum(len(data)/len(df)*sum(-x/len(data)*log(x/len(data),2) for x in data.groupby('play').size()) for key,data in rawOutlooks) print(\"gain of\",column,\"is\",gain) return gain gains = [[x,findGain(x)] for x in ['outlook','temperature','humidity','windy']] value count probability overcast 4 0.285714 rainy 5 0.357143 sunny 5 0.357143 gain of outlook is 0.2467498197744391 value count probability cool 4 0.285714 hot 4 0.285714 mild 6 0.428571 gain of temperature is 0.029222565658954647 value count probability high 7 0.5 normal 7 0.5 gain of humidity is 0.15183550136234136 value count probability False 8 0.571429 True 6 0.428571 gain of windy is 0.04812703040826927 columns=[\"Feature\", \"Gain Score\"]).sort_values(\"Gain Score\")[::-1]) Feature Gain Score outlook 0.24675 humidity 0.151836 windy 0.048127 temperature 0.0292226","title":"Mencari Entropy Target"}]}